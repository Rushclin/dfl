{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rushclin02\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "\n",
    "\n",
    "\n",
    "from src import Range, load_model, tensorboard_runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_weights(model, args):\n",
    "    \"\"\"Charge les poids du modèle en fonction du dispositif disponible.\"\"\"\n",
    "    if torch.cuda.is_available() and args.device == \"cuda\":\n",
    "        model.load_state_dict(torch.load(\n",
    "            'result/DFL_241021_223155/DFL.pt'))\n",
    "        model.to(args.device)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(\n",
    "            'result/DFL_241021_223155/DFL.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "def get_transform(args):\n",
    "    \"\"\"Retourne les transformations d'images.\"\"\"\n",
    "    return Compose([\n",
    "        Resize((args.resize, args.resize)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "def load_and_transform_images(img_paths, transform, device):\n",
    "    \"\"\"Charge et transforme une liste d'images.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        return torch.stack([transform(Image.open(img_path)).to(device) for img_path in img_paths])\n",
    "\n",
    "def make_predictions(model, validation_batch):\n",
    "    \"\"\"Fait des prédictions sur un lot d'images.\"\"\"\n",
    "    model.eval()  # Met le modèle en mode évaluation\n",
    "    pred_logits_tensor = model(validation_batch)\n",
    "    pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n",
    "    return np.argmax(pred_probs, axis=1)\n",
    "\n",
    "def display_results(img_list, predicted_classes, class_names):\n",
    "    \"\"\"Affiche les images avec leurs classes prédites, 4 par ligne.\"\"\"\n",
    "    num_images = len(img_list)\n",
    "    num_cols = 5\n",
    "    # num_rows = (num_images + num_cols - 1) // num_cols  # Calculer le nombre de lignes nécessaires\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols  # Calculer le nombre de lignes nécessaires\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n",
    "    axs = axs.flatten()  # Aplatir la grille pour itérer facilement\n",
    "\n",
    "    for i, img in enumerate(img_list):\n",
    "        ax = axs[i]\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{class_names[predicted_classes[i]]}\")\n",
    "        ax.imshow(img)\n",
    "\n",
    "    # Désactiver les axes pour les sous-graphiques vides\n",
    "    for i in range(num_images, num_rows * num_cols):\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(args):\n",
    "    model, args = load_model(args)\n",
    "    tensorboard_runner(args)\n",
    "\n",
    "    # Liste des chemins des images de validation\n",
    "    validation_img_paths = [\n",
    "        \"./dataset/validation/0/img_37.jpg\",\n",
    "    ]\n",
    "\n",
    "    # Liste des noms des classes correspondants\n",
    "    class_names = [\n",
    "        \"0\",\n",
    "        \"1\",\n",
    "        \"2\",\n",
    "        \"3\",\n",
    "        \"4\",\n",
    "        \"5\",\n",
    "        \"6\",\n",
    "        \"7\",\n",
    "        \"8\",\n",
    "        \"9\"\n",
    "    ]\n",
    "\n",
    "    # Transformer les images\n",
    "    transform = get_transform(args)\n",
    "\n",
    "    # Charger et transformer les images\n",
    "    img_list = [Image.open(img_path) for img_path in validation_img_paths]\n",
    "    validation_batch = load_and_transform_images(validation_img_paths, transform, args.device)\n",
    "\n",
    "    # Faire des prédictions\n",
    "    predicted_classes = make_predictions(model, validation_batch)\n",
    "\n",
    "    # Afficher les résultats\n",
    "    display_results(img_list, predicted_classes, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--resize RESIZE] [--crop CROP] [--imnorm]\n",
      "                             [--hidden_size HIDDEN_SIZE]\n",
      "                             [--model_name {TwoNN,TwoCNN,VGG9,VGG9BN,VGG11,VGG11BN,VGG13,VGG13BN,ResNet10,ResNet18,ResNet34}]\n",
      "                             [--num_classes NUM_CLASSES]\n",
      "                             [--in_channels IN_CHANNELS] [--device DEVICE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\Rushclin02\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3473e9b6d389884ec17e2410b0522b29ae55f7cf6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n",
    "\n",
    "    # Arguments de l'interface en ligne de commande\n",
    "    parser.add_argument('--resize', type=int, default=28)\n",
    "    parser.add_argument('--crop', type=int, default=None)\n",
    "    parser.add_argument('--imnorm', action='store_true')\n",
    "    # parser.add_argument('--randrot', type=int, default=None)\n",
    "    # parser.add_argument('--randhf', type=float, choices=[Range(0., 1.)], default=None)\n",
    "    # parser.add_argument('--randvf', type=float, choices=[Range(0., 1.)], default=None)\n",
    "    # parser.add_argument('--randjit', type=float, choices=[Range(0., 1.)], default=None)\n",
    "    parser.add_argument('--hidden_size', type=int, default=64)\n",
    "\n",
    "    parser.add_argument('--model_name', type=str, choices=[\n",
    "        'TwoNN', 'TwoCNN', 'VGG9', 'VGG9BN', 'VGG11', 'VGG11BN', 'VGG13', 'VGG13BN',\n",
    "        'ResNet10', 'ResNet18', 'ResNet34',\n",
    "    ], default='TwoNN')\n",
    "\n",
    "    parser.add_argument('--num_classes', type=int, default=10)\n",
    "    parser.add_argument('--in_channels', type=int, default=1)\n",
    "    parser.add_argument('--device', type=str, default='cpu')\n",
    "    # parser.add_argument('--log_path', help='Chemin des logs',\n",
    "    #                     type=str, default='./log')\n",
    "    # parser.add_argument('--tb_port', help='TensorBoard',\n",
    "    #                     type=int, default=6006)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
